{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DBSCAN_Statistica.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenzopaolucci/prova/blob/master/DBSCAN_Statistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPrMM8TTPHLi",
        "colab_type": "text"
      },
      "source": [
        "**GENERAZIONE GRIGLIA PER 100 EVENTI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4HznOPKMBQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import cm\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from scipy.stats import norm\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "L = 50                            # Dimensione griglia\n",
        "centers = [[(L-1)*0.5,(L-1)*0.5]] # Centro del blob\n",
        "sigma_noise = 2                   # Sigma del rumore\n",
        "sigma_spatial = 4                 # Sigma della distribuzione spaziale del blob (in pixel)\n",
        "n_samples =3000                   # Numero di segnali generati\n",
        "th = 1.5*sigma_noise\n",
        "\n",
        "os.system('mkdir -p runs')\n",
        "os.chdir('runs') \n",
        "\n",
        "for run in np.arange(0,100,1):\n",
        "\n",
        "    # Genero le griglie\n",
        "\n",
        "    grid = np.zeros((L,L))        # Contiene le \"coordinate\" di ogni pixel, e il valore del segnale\n",
        "    background = np.zeros((L,L))  # Griglia background\n",
        "    signal = np.zeros((L,L))      # Griglia segnale\n",
        "\n",
        "    # Genero il fondo\n",
        "\n",
        "    for i in np.arange(0,L,1):\n",
        "\n",
        "      for j in np.arange(0,L,1):\n",
        "\n",
        "        r = np.random.normal(0,2)\n",
        "        background[i][j] = r\n",
        "\n",
        "    # Genero il blob\n",
        "\n",
        "    X, blob_labels = make_blobs(n_samples=n_samples, centers=centers, cluster_std=sigma_spatial, random_state=0)\n",
        "\n",
        "    # Discretizzazione del blob\n",
        "\n",
        "    for j in np.arange(0,len(X),1):\n",
        "\n",
        "      x = int(np.round(X[j][0]))\n",
        "      y = int(np.round(X[j][1]))\n",
        "\n",
        "      signal[x][y] += 1\n",
        "\n",
        "    # Somma background+segnale\n",
        "\n",
        "    count=0\n",
        "    tot_noise = 0\n",
        "\n",
        "    for j in np.arange(0,L,1):\n",
        "\n",
        "      for i in np.arange(0,L,1):\n",
        "\n",
        "        s = signal[i][j] + background[i][j]\n",
        "\n",
        "        if s >= th:\n",
        "\n",
        "          grid[i][j] = np.round(s)\n",
        "          count += 1\n",
        "\n",
        "        if signal[i][j] == 0 and background[i][j] >= th: # Contiene solo il rumore \"puro\", cioè quello che non si somma al segnale\n",
        "\n",
        "          background[i][j] = np.round(background[i][j])\n",
        "          tot_noise += 1\n",
        "\n",
        "        else:\n",
        "\n",
        "          background[i][j] = 0\n",
        "\n",
        "        if s >= th and signal[i][j] != 0:                # Contiene solo il segnale (a cui ora si è aggiunto il rumore)\n",
        "\n",
        "          signal[i][j] = np.round(s)\n",
        "\n",
        "        else:\n",
        "\n",
        "          signal[i][j] = 0\n",
        "\n",
        "    tot_signal = count - tot_noise                       # Numero totale di pixel di rumore\n",
        "\n",
        "    # Creazione array coordinate per DBSCAN\n",
        "\n",
        "    points_list = []\n",
        "    signal_list = []\n",
        "    background_list= []\n",
        "    phot = 0\n",
        "     \n",
        "    for i in np.arange(0,L,1):\n",
        "\n",
        "      for j in np.arange(0,L,1):\n",
        "\n",
        "        if grid[i][j] != 0:\n",
        "\n",
        "          points_list.append([j,i,grid[i][j]])\n",
        "\n",
        "        if signal[i][j] != 0:\n",
        "          signal_list.append([j,i,signal[i][j]])\n",
        "          phot += signal[i][j]\n",
        "     \n",
        "        if background[i][j] != 0:\n",
        "          background_list.append([j,i,background[i][j]])\n",
        "\n",
        "    points = np.array(points_list)\n",
        "    signal_plot = np.array(signal_list)\n",
        "    background_plot = np.array(background_list)\n",
        "\n",
        "    # Esportazione dataframe\n",
        "\n",
        "    df1 = pd.DataFrame(points, index=None)\n",
        "    df2 = pd.DataFrame(signal_plot, index=None)\n",
        "\n",
        "    df1.to_csv('grid_%d.csv' %run,index_label=False)\n",
        "    df2.to_csv('signal_%d.csv' %run,index_label=False)\n",
        "\n",
        "    #print('%d Background pixels generated over threshold, %d Signal photons (summed with background) over threshold in %d pixels' %(len(background_plot),phot,len(signal_plot)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IF90io31r70",
        "colab_type": "text"
      },
      "source": [
        "**DBSCAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fczlhk5ewy54",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5bfcda89-03c8-4cab-d4d3-a51b537d9f32"
      },
      "source": [
        "for run in np.arange(0,1,1):\n",
        "\n",
        "    # Importazione dataframe\n",
        "\n",
        "    df1 = pd.read_csv('grid_%d.csv' %run)\n",
        "    df2 = pd.read_csv('signal_%d.csv' %run)\n",
        "\n",
        "    points = pd.DataFrame.to_numpy(df1)\n",
        "    signal_plot = pd.DataFrame.to_numpy(df2)\n",
        "\n",
        "    min_eps = 4\n",
        "    max_eps = 8\n",
        "    step_eps = 0.5\n",
        "\n",
        "    purity_highest = []\n",
        "    detection_efficiency_highest = []\n",
        "\n",
        "    for eps in np.arange(min_eps, max_eps, step_eps):\n",
        "\n",
        "        min_min_samples = 2*eps\n",
        "        max_min_samples = 10*eps\n",
        "        step_min_samples = 1\n",
        "\n",
        "        purity_max = 0\n",
        "        detection_efficiency_max = 0\n",
        "\n",
        "        for min_samples in np.arange(min_min_samples, max_min_samples, step_min_samples):\n",
        "\n",
        "\n",
        "            # CLUSTERING\n",
        "\n",
        "            db = DBSCAN(eps, min_samples).fit(points)\n",
        "            core_samples_mask = np.zeros_like(db.labels_,dtype=bool)       # Inizializza un array booleano, della stessa forma di labels_\n",
        "            core_samples_mask[db.core_sample_indices_] = True              # Considera tutti i core trovati da dbscan\n",
        "            labels = db.labels_\n",
        "\n",
        "            n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)    # Conta i cluster, togliendo il  rumore (k=-1)\n",
        "            n_noise_ = list(labels).count(-1)                              # Numero di punti di rumore\n",
        "\n",
        "            # Plot dei cluster individuati\n",
        "\n",
        "            unique_labels = set(labels)\n",
        "            colors = [plt.cm.Spectral(each)\n",
        "            for each in np.linspace(0, 1, len(unique_labels))]             # Sceglie la palette di   colori senza il nero\n",
        "\n",
        "            cluster_efficiency = 0\n",
        "            weight_sum = 0\n",
        "\n",
        "            clusters_points = (labels==-1)  # Seleziona tutti i punti clusterizzati tranne il rumore\n",
        "            \n",
        "            punti = points[~clusters_points]  # Punti clusterizzati da dbscan come segnale\n",
        "           \n",
        "            for k, col in zip(unique_labels, colors):                      # Per ogni cluster, associo un colore\n",
        "            \n",
        "                class_member_mask = (labels == k)                          # Seleziona tutti i punti del cluster k\n",
        "\n",
        "                xy_core = points[class_member_mask & core_samples_mask]    # Solo se è nel cluster E è un core point\n",
        "                xy_border = points[class_member_mask & ~core_samples_mask] # Solo se è nel cluster E non è core  ==  è un edge point del cluster\n",
        "                \n",
        "                # Efficienza della clusterizzazione\n",
        "\n",
        "                phot = 0                                      # Contatore di fotoni\n",
        "                x = 0\n",
        "                y = 0\n",
        "\n",
        "                if k == -1:\n",
        "\n",
        "                  col = [0, 0, 0, 1]                          # Nero per il rumore\n",
        "\n",
        "                else:\n",
        "\n",
        "                  for i in np.arange(0,len(xy_core),1):       # Somme sui pixel contenuti nel cluster k, pesate con il numero di fotoni\n",
        "                    \n",
        "                    x += xy_core[i][0] * xy_core[i][2]\n",
        "                    y += xy_core[i][1] * xy_core[i][2]\n",
        "                    phot += xy_core[i][2]\n",
        "\n",
        "                  for i in np.arange(0,len(xy_border),1):\n",
        "                    \n",
        "                    x += xy_border[i][0] * xy_border[i][2]\n",
        "                    y += xy_border[i][1] * xy_border[i][2]\n",
        "                    phot += xy_border[i][2]\n",
        "\n",
        "                  x /= phot\n",
        "                  y /= phot\n",
        "\n",
        "                  dist = np.sqrt((x-centers[0][0])**2+(y-centers[0][1])**2)\n",
        "                  clust_eff_partial = ((tot_signal - abs(tot_signal-len(xy_core)-len(xy_border)) )/tot_signal)*1/dist\n",
        "\n",
        "                  if clust_eff_partial < 0:\n",
        "                    \n",
        "                    cluster_efficiency += 0\n",
        "\n",
        "                  else:\n",
        "\n",
        "                    cluster_efficiency += clust_eff_partial\n",
        "                  \n",
        "                  weight_sum += 1/dist\n",
        "\n",
        "            # Purezza\n",
        "\n",
        "            if len(punti) > 0:                                # Considero solo le run che trovano segnale\n",
        "\n",
        "              len_sig=len(signal_plot)\n",
        "              len_punti=len(punti)\n",
        "\n",
        "              count_p=0                                       # Conta i pixel trovati da dbscan correttamente\n",
        "              for i in range (len_sig):\n",
        "                for j in range (len_punti):\n",
        "                  if punti[j,0]==signal_plot[i,0] and punti[j,1]==signal_plot[i,1] and punti[j,2]==signal_plot[i,2]:\n",
        "                    count_p+=1\n",
        "                \n",
        "              purity=1-(len(punti)-count_p)/count_p\n",
        "              detection_efficiency=count_p/len(signal_plot)\n",
        "\n",
        "              if purity>purity_max:\n",
        "\n",
        "                purity_max = purity\n",
        "                purity_min_samples = min_samples\n",
        "\n",
        "              if detection_efficiency >= detection_efficiency_max:\n",
        "                \n",
        "                detection_efficiency_max = detection_efficiency\n",
        "                detection_efficiency_min_samples = min_samples\n",
        "\n",
        "            # Efficienza di rivelazione \n",
        "             \n",
        "            if n_clusters_ != 0:\n",
        "\n",
        "              cluster_efficiency /= weight_sum\n",
        "\n",
        "        purity_highest.append([eps,purity_min_samples,purity_max])\n",
        "        detection_efficiency_highest.append([eps,detection_efficiency_min_samples,detection_efficiency_max])\n",
        "\n",
        "    print(purity_highest)\n",
        "    print(detection_efficiency_highest)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.0, 17.0, 0.9732142857142857], [4.5, 24.0, 1.0], [5.0, 27.0, 0.9809523809523809], [5.5, 35.0, 1.0], [6.0, 39.0, 1.0], [6.5, 50.0, 1.0], [7.0, 53.0, 0.9929078014184397], [7.5, 65.0, 1.0]]\n",
            "[[4.0, 8.0, 0.9759036144578314], [4.5, 9.0, 0.9959839357429718], [5.0, 15.0, 0.9959839357429718], [5.5, 15.0, 0.9959839357429718], [6.0, 16.0, 0.9959839357429718], [6.5, 13.0, 1.0], [7.0, 18.0, 1.0], [7.5, 18.0, 1.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}